---
tags:
  - Spark
  - DuckDB
  - Athena
  - SQLite
  - Postgres
  - Backends
---

# Splink's SQL backends: Spark, DuckDB, etc

Splink is a Python library. It implements all data linking computations by generating SQL, and submitting the SQL statements to a backend of the user's choosing for execution.

For smaller input datasets of up to 1-2 million records, users can link data in Python on their laptop using the DuckDB backend. This is the recommended approach because the DuckDB backend is installed automatically when the user installs Splink using `pip install splink`. No additional configuration is needed.

Linking larger datasets requires highly computationally intensive calculations, and generates datasets which are too large to be processed on a standard laptop. For these scenarios, we recommend using one of Splink's big data backend - currently Spark or AWS Athena. When these backends are used, the SQL generated by Splink is sent to the chosen backend for execution.

The Splink code you write is almost identical between backends, so it's straightforward to migrate between backends. Often, it's a good idea to start working using DuckDB on a sample of data, because it will produce results very quickly. When you're comfortable with your model, you may wish to migrate to a big data backend to estimate/predict on the full dataset.

## Choosing a backend

Import the linker from the backend of your choosing, and the backend-specific comparison libraries.

Once you have initialised the `linker` object, there is no difference in the subsequent code between backends.

Note however, that not all comparison functions are available in all backends.
There are tables detailing the available functions for each backend on
the [comparison library API page](../../comparison_library.md) and the [comparison level library API page](../../comparison_level_library.md).

=== ":simple-duckdb: DuckDB"

    ```python
    from splink.duckdb.linker import DuckDBLinker
    import splink.duckdb.comparison_library as cl
    import splink.duckdb.comparison_level_library as cll

    linker = DuckDBLinker(your_args)
    ```

=== ":simple-apachespark: Spark"

    ```python
    from splink.spark.linker import SparkLinker
    import splink.spark.comparison_library as cl
    import splink.spark.comparison_level_library as cll

    linker = SparkLinker(your_args)
    ```

=== ":simple-amazonaws: Athena"

    ```python
    from splink.athena.linker import AthenaLinker
    import splink.athena.comparison_library as cl
    import splink.athena.comparison_level_library as cll

    linker = AthenaLinker(your_args)
    ```

=== ":simple-sqlite: SQLite"

    ```python
    from splink.sqlite.linker import SQLiteLinker
    import splink.sqlite.comparison_library as cl
    import splink.sqlite.comparison_level_library as cll

    linker = SQLiteLinker(your_args)

    ```

=== ":simple-postgresql: PostgreSql"

    ```python
    from splink.postgres.linker import PostgresLinker
    import splink.postgres.comparison_library as cl
    import splink.postgres.comparison_level_library as cll

    linker = PostgresLinker(your_args)

    ```

## Information for specific backends

### :simple-sqlite: SQLite

[**SQLite**](https://www.sqlite.org/index.html) does not have native support for [fuzzy string-matching](../comparisons/comparators.html) functions.
However, some are available for Splink users as python [user-defined functions (UDFs)](../../dev_guides/udfs.html#sqlite):

* [`levenshtein`](../../comparison_level_library.html#splink.comparison_level_library.LevenshteinLevelBase)
* [`damerau_levenshtein`](../../comparison_level_library.html#splink.comparison_level_library.DamerauLevenshteinLevelBase)
* [`jaro`](../../comparison_level_library.html#splink.comparison_level_libraryJaroLevelBase)
* [`jaro_winkler`](../../comparison_level_library.html#splink.comparison_level_library.JaroWinklerLevelBase)

However, there are a couple of points to note:

* These functions are implemented using the [rapidfuzz](https://maxbachmann.github.io/RapidFuzz/) package, which must be installed if you wish to make use of them, via e.g. `pip install rapidfuzz`. If you do not wish to do so you can disable the use of these functions when creating your linker:
```py
linker = SQLiteLinker(df, settings, ..., register_udfs=False)
```
* As these functions are implemented in python they will be considerably slower than any native-SQL comparisons. If you find that your model-training or predictions are taking a large time to run, you may wish to consider instead switching to DuckDB (or some other backend).
